# ChatBERT-ED Base Configuration
# ~110M parameters, trainable on A100 40GB in 4-6 hours

model:
  name: "chatbert-ed-base"
  type: "encoder_decoder"

  encoder:
    # Use pretrained BERT-base as encoder
    pretrained: "bert-base-uncased"
    hidden_size: 768
    num_layers: 12
    num_attention_heads: 12
    intermediate_size: 3072
    freeze: false

  decoder:
    hidden_size: 768
    num_layers: 6
    num_attention_heads: 12
    intermediate_size: 3072
    max_position_embeddings: 256

  cross_attention:
    enabled: true
    num_layers: 6

  vocab_size: 30522
  max_context_length: 512
  max_response_length: 256

training:
  datasets:
    - "daily_dialog"
    - "personachat"
    - "empathetic_dialogues"
  max_turns: 7

  epochs: 5
  batch_size: 8
  gradient_accumulation_steps: 4  # Effective batch size 32
  learning_rate: 3.0e-5
  encoder_learning_rate: 1.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.1

  scheduler: "cosine"

  fp16: true
  gradient_checkpointing: true

  logging_steps: 50
  eval_steps: 500
  save_steps: 1000

  early_stopping_patience: 3

inference:
  max_length: 256
  min_length: 10
  num_beams: 4
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.2
  do_sample: true
