# ChatBERT-IMR (Iterative MLM Refinement) Configuration
# Uses BERT's MLM head for iterative response generation

model:
  name: "chatbert-imr-small"
  type: "iterative_mlm"

  backbone:
    pretrained: "distilbert-base-uncased"
    hidden_size: 768
    num_layers: 6
    num_attention_heads: 12
    intermediate_size: 3072

  vocab_size: 30522
  max_context_length: 256
  max_response_length: 64  # Shorter for iterative refinement

  # Iterative refinement settings
  refinement:
    num_iterations: 10  # Number of refinement steps
    mask_schedule: "confidence"  # Options: confidence, linear, cosine
    initial_mask_ratio: 1.0  # Start fully masked
    min_mask_ratio: 0.0  # End fully unmasked
    temperature_schedule: "linear"  # Anneal temperature during refinement
    initial_temperature: 1.0
    final_temperature: 0.1

training:
  datasets:
    - "daily_dialog"
    - "personachat"
  max_turns: 5

  # Train with variable masking ratios
  mask_ratio_min: 0.15
  mask_ratio_max: 0.95

  epochs: 10
  batch_size: 32
  gradient_accumulation_steps: 1
  learning_rate: 5.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.1

  scheduler: "linear"

  fp16: true
  gradient_checkpointing: false  # Less memory intensive than ED

  logging_steps: 50
  eval_steps: 500
  save_steps: 1000

  early_stopping_patience: 15

inference:
  num_iterations: 15  # More iterations for better quality
  mask_schedule: "confidence"
  temperature: 0.5
  top_k: 10  # More conservative sampling
