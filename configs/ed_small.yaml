# ChatBERT-ED Small Configuration
# ~50M parameters, trainable on A10 24GB in 8-10 hours

model:
  name: "chatbert-ed-small"
  type: "encoder_decoder"

  encoder:
    # Use pretrained DistilBERT as encoder
    pretrained: "distilbert-base-uncased"
    hidden_size: 768
    num_layers: 6
    num_attention_heads: 12
    intermediate_size: 3072
    freeze: false  # Fine-tune encoder

  decoder:
    hidden_size: 512
    num_layers: 4
    num_attention_heads: 8
    intermediate_size: 2048
    max_position_embeddings: 256

  # Cross-attention configuration
  cross_attention:
    enabled: true
    num_layers: 4  # Cross-attention in all decoder layers

  vocab_size: 30522  # BERT tokenizer
  max_context_length: 256
  max_response_length: 128

training:
  # Data
  datasets:
    - "daily_dialog"
    - "personachat"
  max_turns: 5  # Context window of last 5 turns

  # Optimization
  epochs: 5
  batch_size: 16
  gradient_accumulation_steps: 2  # Effective batch size 32
  learning_rate: 5.0e-5
  encoder_learning_rate: 2.0e-5  # Lower LR for pretrained encoder
  weight_decay: 0.01
  warmup_ratio: 0.1

  # Scheduler
  scheduler: "linear"

  # Efficiency
  fp16: true
  gradient_checkpointing: true

  # Logging
  logging_steps: 50
  eval_steps: 500
  save_steps: 1000

  # Early stopping
  early_stopping_patience: 3

inference:
  max_length: 128
  min_length: 5
  num_beams: 4
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.2
  do_sample: true
