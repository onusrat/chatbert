# GPT-2 Baseline Configuration
# 124M parameters, matches ChatBERT-ED training setup for fair comparison

model:
  name: "gpt2-baseline"
  type: "gpt2_baseline"
  pretrained: "gpt2"
  max_length: 512  # context + response combined

training:
  datasets:
    - "daily_dialog"
    - "personachat"
  max_turns: 5

  epochs: 5
  batch_size: 16
  gradient_accumulation_steps: 2  # Effective batch size 32
  learning_rate: 5.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.1

  scheduler: "linear"

  fp16: true

  logging_steps: 50
  eval_steps: 500
  save_steps: 1000

  early_stopping_patience: 3

inference:
  max_length: 128
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.2
